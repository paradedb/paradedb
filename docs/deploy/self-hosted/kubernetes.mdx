---
title: Kubernetes
---

Kubernetes is the recommended way to run ParadeDB in production. To deploy on Kubernetes, we will be using the [ParadeDB Helm Chart](https://github.com/paradedb/charts),
which can wrap around either the ParadeDB Community or Enterprise binaries. The chart is also available on [Artifact Hub](https://artifacthub.io/packages/helm/paradedb/paradedb).

## Prerequisites

This guide assumes you have installed [Helm](https://helm.sh/docs/intro/install/) and have a Kubernetes cluster running v1.25+.
For local testing, we recommend [Minikube](https://minikube.sigs.k8s.io/docs/start/).

## Install the Prometheus Stack

The ParadeDB Helm chart supports monitoring via Prometheus and Grafana. To enable this, you need to have the Prometheus CRDs installed before installing the CloudNativePG operator. If you do not yet have the Prometheus CRDs installed on your Kubernetes cluster, you can install it with:

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm upgrade --atomic --install prometheus-community \
--create-namespace \
--namespace prometheus-community \
--values https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/main/docs/src/samples/monitoring/kube-stack-config.yaml \
prometheus-community/kube-prometheus-stack
```

## Install the CloudNativePG Operator

Skip this step if the CloudNativePG operator is already installed in your cluster. If you do not wish to monitor your cluster, omit the `--set` commands.

```bash
helm repo add cnpg https://cloudnative-pg.github.io/charts
helm upgrade --atomic --install cnpg \
--create-namespace \
--namespace cnpg-system \
--set monitoring.podMonitorEnabled=true \
--set monitoring.grafanaDashboard.create=true \
cnpg/cloudnative-pg
```

## Start a ParadeDB CNPG Cluster

Create a `values.yaml` and configure it to your requirements. Here is a basic example:

<CodeGroup>

```yaml ParadeDB Community
type: paradedb
mode: standalone

cluster:
  instances: 1
  storage:
    size: 256Mi
```

```yaml ParadeDB Enterprise
type: paradedb-enterprise
mode: standalone

cluster:
  instances: 1
  storage:
    size: 256Mi
```

</CodeGroup>

<Note>
  If you are using ParadeDB Enterprise, `instances` should be set to a number
  greater than `1` for [high
  availability](/deploy/self-hosted/high-availability).
</Note>

Next, create a namespace for this step or use an existing namespace. The namespace can be any value.

```bash
kubectl create namespace <your-namespace>
```

For ParadeDB Enterprise, you should have received an enterprise Docker username and personal access token. The following step passes these
credentials to Kubernetes and should be skipped if you are deploying ParadeDB Community.

```bash ParadeDB Enterprise
kubectl create secret docker-registry paradedb-enterprise-registry-cred
--namespace <your-namespace>
--docker-server="https://index.docker.io/v1/"
--docker-username="<enterprise_docker_username>"
--docker-password="<enterprise_docker_access_token>"
```

Finally, launch the ParadeDB cluster.

```bash
helm repo add paradedb https://paradedb.github.io/charts
helm upgrade --atomic --install paradedb \
--namespace <your-namespace> \
--values values.yaml \
--set cluster.monitoring.enabled=true \
paradedb/paradedb
```

## Connect to the Cluster

The command to connect to the primary instance of the cluster will be printed in your terminal. If you do not modify any settings, it will be:

```bash
kubectl --namespace paradedb exec --stdin --tty services/paradedb-rw -- bash
```

This will launch a Bash shell inside the instance. You can connect to the ParadeDB database via `psql` with:

```bash
psql -d paradedb
```

## Connect to the Grafana Dashboard

To connect to the Grafana dashboard for your cluster, we suggested port forwarding the Kubernetes service running Grafana to localhost:

```bash
kubectl --namespace prometheus-community port-forward svc/prometheus-community-grafana 3000:80
```

You can then access the Grafana dasbhoard at [http://localhost:3000/](http://localhost:3000/) using the credentials `admin` as username
and `prom-operator` as password. These default credentials are defined in the [`kube-stack-config.yaml`](https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/main/docs/src/samples/monitoring/kube-stack-config.yaml)
file used as the `values.yaml` file in [Installing the Prometheus CRDs](#installing-the-prometheus-stack) and can be modified by providing
your own `values.yaml` file. A more detailed guide on monitoring the cluster can be found in the [CloudNativePG documentation](https://cloudnative-pg.io/documentation/current/monitoring/).
