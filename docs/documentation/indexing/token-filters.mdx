---
title: Token Filters
noindex: true
---

Token filters apply additional processing to tokens after they have been created.

## Stemmer

Stemming is the process of reducing words to their root form. In English, for example, the root form of `running` and `runs` is `run`. The `stemmer` filter can
be applied to any tokenizer.

```sql
CREATE INDEX search_idx ON mock_items
USING bm25 (id, description)
WITH (
    key_field='id',
    text_fields='{
        "description": {"tokenizer": {"type": "default", "stemmer": "English"}}
    }'
);
```

<ParamField body="stemmer">
  Available stemmers are `Arabic`, `Danish`, `Dutch`, `English`, `Finnish`,
  `French`, `German`, `Greek`, `Hungarian`, `Italian`, `Norwegian`,
  `Portuguese`, `Romanian`, `Russian`, `Spanish`, `Swedish`, `Tamil`, and
  `Turkish`.
</ParamField>

## Remove Long

The `remove_long` filter removes all tokens longer than a fixed number of bytes. If not specified,
`remove_long` defaults to `255`.

```sql
CREATE INDEX search_idx ON mock_items
USING bm25 (id, description)
WITH (
    key_field='id',
    text_fields='{
        "description": {"tokenizer": {"type": "default", "remove_long": 255}}
    }'
);
```

## Lowercase

The `lowercase` filter lowercases all tokens. If not specified, `lowercase` defaults to `true`.

```sql
CREATE INDEX search_idx ON mock_items
USING bm25 (id, description)
WITH (
    key_field='id',
    text_fields='{
        "description": {"tokenizer": {"type": "default", "lowercase": false}}
    }'
);
```

## Stopwords Language

<Note>This filter is not supported for the ngram tokenizer.</Note>

`stopwords_language` removes common "stop words" for a specific language from the original text before tokenization.

```sql
CREATE INDEX search_idx ON mock_items
USING bm25 (id, description)
WITH (
    key_field='id',
    text_fields='{
        "description": {"tokenizer": {"type": "default", "stopwords_language": "English"}}
    }'
);
```

<ParamField body="stopwords_language">
  Available languages are `Danish`, `Dutch`, `English`, `Finnish`, `French`,
  `German`, `Hungarian`, `Italian`, `Norwegian`, `Portuguese`, `Russian`,
  `Spanish`, and `Swedish`.
</ParamField>

## Custom Stopwords

<Note>This filter is not supported for the ngram tokenizer.</Note>

`stopwords` removes custom words from the original text before tokenization.

```sql
CREATE INDEX search_idx ON mock_items
USING bm25 (id, description)
WITH (
    key_field='id',
    text_fields='{
        "description": {"tokenizer": {"type": "default", "stopwords": ["shoes", "boots"]}}
    }'
);
```

## ASCII Folding

The ASCII folding filter strips away diacritical marks (accents, umlauts, tildes, etc.) while leaving the base character intact.

```sql
CREATE INDEX search_idx ON mock_items
USING bm25 (id, description)
WITH (
    key_field='id',
    text_fields='{
        "description": {"tokenizer": {"type": "default", "ascii_folding": true}}
    }'
);
```

## Remove Whitespace

The `remove_whitespace` filter removes tokens that consist entirely of whitespace characters (spaces, tabs, newlines, etc.). This is particularly useful for tokenizers like Jieba and Lindera that may produce whitespace-only tokens.

```sql
CREATE INDEX search_idx ON mock_items
USING bm25 (id, description)
WITH (
    key_field='id',
    text_fields='{
        "description": {"tokenizer": {"type": "jieba", "remove_whitespace": true}}
    }'
);
```

The filter works with all tokenizers. Here are some examples:

```sql
-- With Jieba (Chinese)
SELECT '富裕 劳动力'::pdb.jieba('remove_whitespace=true')::text[];
-- Returns: {富裕,劳动,动力,劳动力} (no space token)

-- With Lindera (Chinese)
SELECT '富裕 劳动力'::pdb.lindera('chinese', 'remove_whitespace=true')::text[];
-- Returns: {富裕,劳动力} (no space token)

-- With Lindera (Korean)
SELECT '아름다운 우리나라'::pdb.lindera('korean', 'remove_whitespace=true')::text[];
-- Returns Korean tokens without spaces
```

<Note>
  The `remove_whitespace` filter detects all Unicode whitespace characters, not just spaces. This includes tabs, newlines, and other Unicode whitespace as defined by the Unicode standard.
</Note>
