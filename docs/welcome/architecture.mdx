---
title: Architecture
---

All of the functionalities of ParadeDB are shipped as a Postgres extension called `pg_search`. `pg_search` introduces a new
index, along with custom query execution methods, to Postgres.

## Custom Index

![ParadeDB BYOC Topology](/images/architecture_indexam.png)

In Postgres, indexes allow the underlying Postgres table (which Postgres calls a *heap table*) to be represented in alternative data structures optimized for different types
of queries. For instance, the built-in Postgres B-tree index is used for point lookups and range scans.

ParadeDB introduces a custom index called the BM25 index that's designed full text search and analytics.

### Index Updates

When a table row is inserted or updated, Postgres automatically notifies the index. These changes are recorded as part of the current transaction, ensuring that index updates are real-time and transactional.

### Parallel Custom Scan

In Postgres, custom scans allow extensions to define new execution nodes that can run custom logic — such as filters or aggregations — natively within the query executor.
ParadeDB's custom scan allows filters, "top N" queries, and aggregates to be pushed down directly into the BM25 index.

## Underlying Data Structures

### Inverted Index

Full text search in ParadeDB is backed by an inverted index.

An inverted index is a structure that maps each term (i.e., tokenized word) to a list of documents that contain that term, along with metadata like term frequency and document frequency.
This structure allows ParadeDB to efficiently retrieve all documents matching a particular search term or phrase without scanning the entire table.

### Columnar Index

Alongside the inverted index, ParadeDB also maintains a structure that stores fields in a column-oriented format.

### LSM Tree

The BM25 index is arranged as Log-Structured Merge (LSM) tree on disk. An LSM tree is a write-optimized data structure commonly used in systems like RocksDB and Cassandra.

The core idea behind an LSM tree is to turn random writes into sequential ones. Incoming writes are first stored in an in-memory buffer called a memtable, which is fast to update. Once the memtable fills up, it is flushed to disk as a sorted, immutable segment file (often called an SSTable).

These segment files are organized by size into layers or levels. Newer data is written to the topmost layer. Over time, data is gradually pushed down into lower levels through a process called compaction, where data from smaller segments is merged, deduplicated, and rewritten into larger segments.

## Guarantees

ParadeDB inherits the strong guarantees of the underlying Postgres engine.

### Strong Consistency

All reads and writes go through Postgres’ transaction engine, ensuring serializability and consistency across indexes and base tables. Search results are always consistent with the current transactional state—no eventual consistency, lag, or stale reads.

### ACID

Search indexes participate fully in Postgres transactions. Inserts, updates, and deletes to indexed columns are atomic and durable. If a transaction is rolled back, associated index changes are also rolled back.

### SQL Syntax

ParadeDB extends standard SQL with additional functions and operators for search and analytics, but remains fully compatible with core SQL syntax. You can use search indexes in any query context—`SELECT`, `JOIN`, `CTE`, `VIEW`, `MATERIALIZED VIEW`—just like any native Postgres index.

### Postgres WAL Compatible

All index changes are logged via Postgres' Write-Ahead Log (WAL), ensuring crash safety and support for physical and logical replication. ParadeDB indexes replicate and recover like any other Postgres relation, requiring no special setup or coordination.

## Tradeoffs

### Why Postgres Needs a New Index

Postgres' existing indexes have key limitations for modern search use cases:

- Filtered/boolean query support is limited and inefficient
- Not optimized for high-ingest workloads
- No support for query parallelization
- Lacks built-in BM25 scoring

In addition, PostgreSQL has limited support for efficient aggregations. Clauses like COUNT, GROUP BY, and ORDER BY are not optimized:

- Postgres must materialize the full result set and perform aggregation in memory, which becomes a bottleneck at scale
- There is no support for aggregation pushdown into Postgres' built-in indexes
- Batch reads are inefficient because Postgres does not provide a way to read index data in a columnar or vectorized format

### Optimized for OLTP and OLAP

###