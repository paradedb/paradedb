Betting on Postgres Unified Retrieval Engine Wins
By Ankit Mittal
I recently sat down with Benjamin & Eldad on the Data Engineering Show to talk about a migration that raised a lot of eyebrows: moving search at Instacart from Elasticsearch to Postgres. Since then, the feedback has ranged from "You're crazy" to "Finally, someone said it."
The industry has spent the last decade unbundling the database. We were told we needed a specialized tool for everything: Vector databases for embeddings, Neo4j for graphs, Search databases for keywords, and Postgres just for the "boring" relational stuff. But the game has changed. Retrieval is no longer just about finding records and “search” for humans; it is now the backbone of modern AI and RAG systems.
Trying to build this intelligent infrastructure on a fractured stack has created an impossible-to-debug nightmare of syncing multiple data systems.
Here is my controversial take: The era of the fragmented search stack is over. We are entering the age of PURE (Postgres Unified Retrieval Engine), and it lives entirely inside the database.
What is a Unified Retrieval Engine?
We are now witnessing a convergence toward the "Unified Retrieval Engine." This paradigm posits that PostgreSQL due to its extensible architecture can serve as a single, ACID-compliant substrate for multimodal retrieval. By leveraging advanced extensions like ParadeDB for BM25 lexical scoring, pgvector for high-dimensional semantic search, and Apache-AGE’s query capabilities for graph traversal, architects can collapse the complete RAG retrieval stack.
Rust in the Database
The common critique I hear is: "Postgres can't handle this much math”!
This is a complete misunderstanding since pushing compute down to the data layer is a known high efficiency computing pattern. PostGIS was the OG extension that has been handling geo-related queries for over a decade, long before pgvector and ParadeDB.
While it is true that, PL/pgSQL is too slow for complex math and one had to write the custom code in C for it to be performant. The game has changed significantly with pgrx. We can now write high-performance Rust, compiled to native machine code running inside the Postgres process.
Components of Unified Retrieval & Search Engine
Core Retrieval

Semantic retrieval has been the workhorse of RAG workloads. Pgvector works well for most of such workloads unless complex category filtering & large dims are needed. My personal sweet spot is 384 dimensional vectors.

Lexical or BM25 retrieval that comes with ParadeDB is back in vogue as engineers are recognizing its value for improving precision. Compared to vector retrieval, that brings in more recall & compromised precision. A good mental model for high recall is high intelligence & high hallucination while a high precision is exhaustive-ness of memory and instructions.

The third pillar of the unified retrieval engine is the graph. Often used in RAG applications and called GraphRag. In high-quality datasets, explicit relationships (citations, social connections, product taxonomies) provide the strongest signal of relevance. While specialized graph databases like Neo4j use property graph models, PostgreSQL can perform sophisticated graph traversal using its native relational engine and Recursive Common Table Expressions. Alternatively, apache-age extension can be used for advanced graph traversals.
Post Retrieval ie Re-ranking
These three retrieval methods (vector, lexical via bm25 & graph traversal) can be combined with powerful post retrieval re-rankings or boostings
Hot + Nuclear Decay (Reddit  style algo): Re-ranking on nuclear fission-like score that decays over time while also giving weight to its voting pattern
Reciprocal Rank Fusion (RRF) : Normalize scores of multiple retrieval sorted sets to produce a single sorted set
Personalized re-rankers - Biencoders or two tower model embeddings can be used to re-rank  retrieved documents for personalized user signals
CTR or click through rate boosting can be natively included in re-ranker by storing materialized views of historical click-through rate.
Pre Retrieval Query transformations
[!Example] For a search query of “Car” .. query expansion leads to “Auto OR Car OR Vehicle OR Sedan." This expansion is injected directly into ParadeDB or all three retrievals for increasing recall.
Spell checking, proximity or slop queries.
Query expansion via lookup or Semantic knowledge graphs*
Advanced tokenization (including unicode & Asian languages)
Query expansion by running a pgvector scan over corpus tokens. Some use cases have completely removed their pgvector usage during retrieval while bringing forth “semnatic-ness” in query expansion phase.
*knowledge graphs are coming back. LLMs have drastically lowered the costs of creating a high quality dataset. They can be used for grounding a 2-hop thinking retrieval among other usecases.
2-hop retrieval
Once we cover the above basics, a RAG style multi hop reasoning model i.e. 2-hop retrieval also becomes possible. With first pass of BM25 and vector retrieval generating docs only for related terms and then those related terms being used for second round of retrieval that underpins a reasoning variant of a RAG system.
Conclusion
This approach reduces data movement, guarantees consistency, and allows for sophisticated "Zero-ETL" search and retrieval workflows. It transforms the database from a passive storage container into an active, intelligent retrieval system.
It allows you to move faster, debug easier, and sleep better.

Postgres vs. Elasticsearch: Instacart's Unexpected Winner in High-Stakes Search with Ankit Mittal
Building Intelligent Applications with Graph-Based RAG on PostgreSQL | POSETTE 2025
